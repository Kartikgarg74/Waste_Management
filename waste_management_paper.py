# -*- coding: utf-8 -*-
"""WASTE MANAGEMENT PAPER

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/waste-management-paper-19c54f67-02a0-41ee-a240-8519f1fff1d8.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20250109/auto/storage/goog4_request%26X-Goog-Date%3D20250109T105118Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D2d853d6ace29eed991cd1be71c7b129f14c8f09444e19c0c1dce510fa855aaaef9da34f32c566701728601fc3773826603d19993311f281cc58ecde177a884716af48dee2241b09ad00394d111af5bd86a0eb0e348791cee1ddea2cf9cb608efcecd4c0c9f973f15f85052894010639c5d73860cfe9852244a285795f4d0aadcb566c357fd81ce648dee3cf249c65b196a4b1a987424a81bf1926711a8958947267ec7b954996403233307cdf51d5a5921bfa6b253ea7258a07b6dc83bd9bcbfb42b42c4493b717ec78b24e12f6355549e95cf44587ebdb6385322525b53b878cb898c5b2293ac155d3bc34277e1aa32093ae731d9a8febf1c2be2a2bed2ade5
"""

# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE
# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.
import kagglehub
kagglehub.login()

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.

kartikgarg74_waste_dataset_path = kagglehub.dataset_download('kartikgarg74/waste-dataset')
kartikgarg74_shp_file_path = kagglehub.dataset_download('kartikgarg74/shp-file')

print('Data source import complete.')

"""## Import Commands"""

import pandas as pd
import os
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import geopandas as gpd
import joblib
from sklearn.model_selection import train_test_split
from sklearn.linear_model import Ridge, ElasticNet, BayesianRidge, HuberRegressor, LinearRegression, Lasso
from sklearn.svm import SVR
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import GradientBoostingRegressor, ExtraTreesRegressor, RandomForestRegressor, BaggingRegressor, AdaBoostRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.neural_network import MLPRegressor
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.linear_model import TheilSenRegressor, RANSACRegressor
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.linear_model import BayesianRidge, TheilSenRegressor
from sklearn.gaussian_process import GaussianProcessRegressor

"""## Data Loading"""

# Step 1: Load the dataset
data_path = '/kaggle/input/waste-dataset/GWMO2024 data set june2024 - national.csv'
data = pd.read_csv(data_path)

"""## Data Insigths"""

print(data.columns)
print(data.info())
print(data.head())
print(data['Region'].unique())
print(data.shape)

"""## Data Pre Processing"""

# Convert numeric columns with commas to numeric
for col in ['MSW 2020', 'MSW 2030', 'MSW 2040', 'MSW 2050', 'Population 2020', 'Population 2030', 'Population 2040', 'Population 2050', 'GDP per capita (PPP international USD)']:
    data[col] = data[col].replace({',': ''}, regex=True).astype(float)
# Handle missing values
# Fill numeric columns with mean and categorical columns with mode
for col in data.columns:
    if data[col].dtype in ['float64', 'int64']:
        data[col].fillna(data[col].mean(), inplace=True)
    else:
        data[col].fillna(data[col].mode()[0], inplace=True)

data.info()

"""## Feature Engineering"""

data['Population Growth Rate'] = (data['Population 2050'] - data['Population 2020']) / data['Population 2020']
data['MSW Growth Rate'] = (data['MSW 2050'] - data['MSW 2020']) / data['MSW 2020']
data['Waste Intensity'] = data['MSW 2020'] / data['GDP per capita (PPP international USD)']
data['Urbanization Effect'] = data['Urbanization rate (%)'] * (data['MSW 2020'] / data['Population 2020'])
data['HDI Adjusted Waste'] = data['MSW 2020'] * data['Human Development Index']

"""## Correlation Plot"""

# Step 4: Fix Correlation Plot Error (Region-Wise Correlation Plots)
def regionwise_correlation(data, regions, columns):
    for region in regions:
        plt.figure(figsize=(10, 6))
        region_data = data[data['Region'] == region]
        # Select only the specified columns for correlation
        selected_data = region_data[columns]
        numeric_cols = selected_data.select_dtypes(include=[np.number])
        corr = numeric_cols.corr()

        sns.heatmap(corr, annot=True, fmt='.2f', cmap='coolwarm', cbar=True)
        plt.title(f'Feature Correlation Heatmap - {region}')
        plt.show()

# List of regions
regions = [
    'East Asia', 'Central and South Asia', 'North America', 'South America',
    'Sub-Saharan Africa', 'East Europe', 'Central America and the Caribbean',
    'West Asia and North Africa', 'West Europe', 'North Europe', 'South Europe',
    'Australia and New Zealand', 'Oceania'
]

# Columns for correlation analysis
columns = [
    'MSW 2020', 'MSW kg/per capita/day 2020', 'Population 2020',
    'GDP per capita (PPP international USD)', 'Urbanization rate (%)',
    'Human Development Index'
]

regionwise_correlation(data, regions, columns)

"""## Data Plot"""

# Create interactive scatter plot
fig = px.scatter(
    data,
    x="Population 2020",
    y="MSW 2020",
    color="GDP per capita (PPP international USD)",
    size="Urbanization rate (%)",
    hover_name="Name",
    title="Population 2020 vs. MSW 2020",
    labels={"Population 2020": "Population 2020", "MSW 2020": "MSW 2020"},
)

# Adjust layout for better visibility
fig.update_layout(
    coloraxis_colorbar=dict(title="GDP per capita (USD)"),
    xaxis=dict(range=[0, 5e8]),  # Adjust zoom range for x-axis if needed
    yaxis=dict(range=[0, 5e4]),  # Adjust zoom range for y-axis if needed
)
fig.show()

"""## Countries with High MSW"""

shapefile_path = "/kaggle/input/shp-file/ne_110m_admin_0_countries.shp"
world = gpd.read_file(shapefile_path)
# Step 1: Load a world shapefile
# Use GeoPandas' built-in dataset for world boundaries
world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))

threshold = 50000  # Adjust this value as per your analysis
data['Risk_Level'] = ['High Risk' if x >= threshold else 'Low Risk' for x in data['MSW 2020']]

# Step 4: Preprocess for merging
# Ensure country names match between GeoPandas' world dataset and your dataset
# Check for mismatches and fix them if necessary
# Example: If there are mismatches in 'Name' and 'name' columns, handle them here
data['Name'] = data['Name'].replace({
    'United States of America': 'United States',
    'Russia': 'Russian Federation',
    # Add more replacements if necessary
})

# Step 5: Merge the world map with your dataset
# This joins the world shapefile with your dataset based on country names
merged = world.merge(data, left_on="name", right_on="Name", how="left")

# Step 6: Plot the "Risk_Level" column
fig, ax = plt.subplots(1, 1, figsize=(15, 10))

# Plot the data, using the 'Risk_Level' column for coloring
# Use a colormap to distinguish High Risk and Low Risk areas
merged.plot(column='Risk_Level', legend=True, cmap='RdYlGn', ax=ax)

# Add a title and display the plot
plt.title("Risk Levels by Country (MSW 2020)", fontsize=16)
plt.show()

"""# Define features and target"""

X = data[['Region', 'Group', 'GDP per capita (PPP international USD)',
          'Urbanization rate (%)', 'Human Development Index','Population 2020']]
y = data['MSW 2020']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Preprocessing: encode categorical and scale numeric features
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), ['Population 2020', 'GDP per capita (PPP international USD)',
                                   'Urbanization rate (%)', 'Human Development Index']),
        ('cat', OneHotEncoder(), ['Region', 'Group'])
    ])

"""## Model Training"""

# Define models to be implemented
models = {
    "Linear Regression": LinearRegression(),
    "Ridge Regression": Ridge(),
    "ElasticNet Regression": ElasticNet(),
    "Lasso Regression": Lasso(),
    "Logistic Regression": LogisticRegression(),
    "Random Forest Regressor": RandomForestRegressor(),
    "Decision Tree Regressor": DecisionTreeRegressor(),
    "ElasticNet Regression": ElasticNet(),
    "Huber Regression": HuberRegressor(),
    "Support Vector Regression (RBF)": SVR(kernel='rbf'),
    "Gradient Boosting Regressor": GradientBoostingRegressor(),
    "Extra Trees Regressor": ExtraTreesRegressor(),
    "k-Nearest Neighbors Regressor": KNeighborsRegressor(),
    "Bagging Regressor": BaggingRegressor(),
    "AdaBoost Regressor": AdaBoostRegressor(),
    "MLP Regressor": MLPRegressor(max_iter=500),
    "RANSAC Regressor": RANSACRegressor()
}
save_path = "/kaggle/working/"
eval_results = []
# Loop through models, train, evaluate, and plot
for name, model in models.items():
    print(f"\n{name}\n")

    # Create a pipeline for preprocessing and the model
    pipeline = Pipeline(steps=[('preprocessor', preprocessor),
                                ('model', model)])

    # Train the model
    pipeline.fit(X_train, y_train)
    y_pred = pipeline.predict(X_test)

    # Save the model
    #model_save_path = f'{save_path}{name.replace(" ", "_").lower()}_model.pkl'
    #joblib.dump(pipeline, model_save_path)
    #print(f"{name} model saved successfully at {model_save_path}")

    # Evaluate the model
    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    mae = mean_absolute_error(y_test, y_pred)
    rmse = np.sqrt(mse)
    print(f"Mean Squared Error (MSE): {mse:.4f}")
    print(f"R² Score: {r2:.4f}")
    print(f"Mean Absolute Error (MAE): {mae:.4f}")
    print(f"Root Mean Squared Error (RMSE): {rmse:.4f}")
    print("-" * 50)

    # Store the evaluation results
    eval_results.append({
        'Model': name,
        'MSE': mse,
        'R²': r2,
        'MAE': mae,
        'RMSE': rmse
    })

    # Combine results for visualization
    X_test_df = pd.DataFrame(X_test, columns=X.columns)
    y_test_df = pd.DataFrame(y_test).reset_index(drop=True)
    y_pred_df = pd.DataFrame(y_pred, columns=['Predicted MSW 2020'])
    results_df = pd.concat([X_test_df.reset_index(drop=True), y_test_df, y_pred_df], axis=1)
    results_df['Residuals'] = results_df['MSW 2020'] - results_df['Predicted MSW 2020']

    # Plotting
    plt.figure(figsize=(16, 8))

    # 1. Actual vs Predicted Values
    plt.subplot(1, 2, 1)
    sns.scatterplot(data=results_df, x='MSW 2020', y='Predicted MSW 2020', hue='Region')
    plt.plot([results_df['MSW 2020'].min(), results_df['MSW 2020'].max()],
             [results_df['MSW 2020'].min(), results_df['MSW 2020'].max()],
             color='red', linestyle='--', linewidth=2)
    plt.title(f'{name} - Actual vs Predicted MSW 2020')
    plt.xlabel('Actual MSW 2020')
    plt.ylabel('Predicted MSW 2020')
    plt.legend(title='Region', bbox_to_anchor=(1.05, 1), loc='upper left')

    # 2. Residual Plot
    plt.subplot(1, 2, 2)
    sns.histplot(data=results_df, x='Residuals', kde=True, bins=20, color='blue')
    plt.axvline(0, color='red', linestyle='--')
    plt.title(f'{name} - Residuals Distribution')
    plt.xlabel('Residuals')
    plt.ylabel('Frequency')

    # Adjust layout to prevent overlap
    plt.tight_layout()

    # Save the plot
    plot_filename = f'{save_path}/{name.replace(" ", "_").lower()}_plot.png'
    plt.savefig(plot_filename)
    print(f"Plot saved as {plot_filename}")

    # Display the plot
    plt.show()

# Define a preprocessing pipeline
numeric_features = X.select_dtypes(include=['int64', 'float64']).columns
categorical_features = X.select_dtypes(include=['object']).columns

numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='mean')),
    ('scaler', StandardScaler())
])

categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))  # Convert to dense format directly
])

preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_features),
        ('cat', categorical_transformer, categorical_features)
    ]
)

# List of models to evaluate
models = {
    "Bayesian Ridge": BayesianRidge(),
    "Gaussian Process Regressor": GaussianProcessRegressor(),
    "Theil-Sen Regressor": TheilSenRegressor()
}

# Define the path to save models
save_path = "/kaggle/working/"

# Loop through models, train, evaluate, and plot results
for name, model in models.items():
    # Define the pipeline
    pipeline = Pipeline(steps=[
        ('preprocessor', preprocessor),
        ('regressor', model)
    ])

    # Train the model
    pipeline.fit(X_train, y_train)

    # Get predictions
    y_pred = pipeline.predict(X_test)

    # Evaluate the model
    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    mae = mean_absolute_error(y_test, y_pred)
    rmse = np.sqrt(mse)

    # Print evaluation metrics
    print(f"Model: {name}")
    print(f"Mean Squared Error (MSE): {mse:.4f}")
    print(f"R² Score: {r2:.4f}")
    print(f"Mean Absolute Error (MAE): {mae:.4f}")
    print(f"Root Mean Squared Error (RMSE): {rmse:.4f}")
    print("-" * 50)

    # Save the model
    #model_save_path = f'{save_path}{name.replace(" ", "_").lower()}_model.pkl'
    #joblib.dump(pipeline, model_save_path)
    #print(f"{name} model saved successfully at {model_save_path}")

    # Store the evaluation results
    eval_results.append({
        'Model': name,
        'MSE': mse,
        'R²': r2,
        'MAE': mae,
        'RMSE': rmse
    })

    # Combine results for visualization
    X_test_df = pd.DataFrame(X_test, columns=X.columns)
    y_test_df = pd.DataFrame(y_test).reset_index(drop=True)
    y_pred_df = pd.DataFrame(y_pred, columns=['Predicted MSW 2020'])
    results_df = pd.concat([X_test_df.reset_index(drop=True), y_test_df, y_pred_df], axis=1)
    results_df['Residuals'] = results_df['MSW 2020'] - results_df['Predicted MSW 2020']

    # Plotting
    plt.figure(figsize=(16, 8))

    # 1. Actual vs Predicted Values
    plt.subplot(1, 2, 1)
    sns.scatterplot(data=results_df, x='MSW 2020', y='Predicted MSW 2020', hue='Region')
    plt.plot([results_df['MSW 2020'].min(), results_df['MSW 2020'].max()],
             [results_df['MSW 2020'].min(), results_df['MSW 2020'].max()],
             color='red', linestyle='--', linewidth=2)
    plt.title(f'{name} - Actual vs Predicted MSW 2020')
    plt.xlabel('Actual MSW 2020')
    plt.ylabel('Predicted MSW 2020')
    plt.legend(title='Region', bbox_to_anchor=(1.05, 1), loc='upper left')

    # 2. Residual Plot
    plt.subplot(1, 2, 2)
    sns.histplot(data=results_df, x='Residuals', kde=True, bins=20, color='blue')
    plt.axvline(0, color='red', linestyle='--')
    plt.title(f'{name} - Residuals Distribution')
    plt.xlabel('Residuals')
    plt.ylabel('Frequency')

    plt.tight_layout()

    # Save the plot
    plot_filename = f'{save_path}/{name.replace(" ", "_").lower()}_plot.png'
    plt.savefig(plot_filename)
    print(f"Plot saved as {plot_filename}")
    plt.show()

"""# Convert evaluation results to DataFrame and save as CSV"""

eval_results_df = pd.DataFrame(eval_results)
eval_results_df.to_csv(f'{save_path}model_evaluation_results.csv', index=False)
print(f"Evaluation results saved at: {save_path}model_evaluation_results.csv")